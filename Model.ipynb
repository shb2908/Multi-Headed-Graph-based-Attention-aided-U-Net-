{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZBz4Js_WmFj",
        "outputId": "b683ffe9-704c-4ae8-9c10-978218671ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b041422f"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import keras.backend as K\n",
        "import tensorflow.keras.layers as L\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, MaxPool2D, Add, Dropout, Concatenate, Conv2DTranspose, Dense, Reshape, Flatten, Softmax, Lambda, UpSampling2D, AveragePooling2D, Activation, BatchNormalization, GlobalAveragePooling2D, SeparableConv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "!pip install tensorflow-wavelets\n",
        "import tensorflow_wavelets.Layers.DWT as DWT\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import Recall, Precision, MeanIoU\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "import time\n",
        "import zipfile\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn8ynCb4q7kj"
      },
      "outputs": [],
      "source": [
        "tissue_train = sorted(os.listdir(os.path.join(train_path, \"TissueImages\")))\n",
        "mask_train = sorted(os.listdir(os.path.join(train_path, \"GroundTruth\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9BwZf5rrfYE"
      },
      "outputs": [],
      "source": [
        "tissue_test = sorted([\n",
        "    file for file in os.listdir(os.path.join(test_path, \"TissueImages\"))\n",
        "    if file.lower().endswith('.png')\n",
        "])\n",
        "mask_test = sorted(os.listdir(os.path.join(test_path, \"GroundTruth\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uR8UJbL-CAg"
      },
      "outputs": [],
      "source": [
        "def apply_otsu_thresholding(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    threshold, thresholded = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    #cv2.imwrite(image_path, thresholded)\n",
        "    return threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbHD9tDj-8n9"
      },
      "outputs": [],
      "source": [
        "def create_train(tissue_train, mask_train,batch):\n",
        "  def parse_images(tissue_train, mask_train):\n",
        "\n",
        "    tissue_file_str = tf.strings.join([train_path, \"TissueImages/\", tissue_train])\n",
        "    mask_file_str = tf.strings.join([train_path, \"GroundTruth/\", mask_train])\n",
        "\n",
        "    tissue_image = tf.io.read_file(tissue_file_str)\n",
        "    mask_image = tf.io.read_file(mask_file_str)\n",
        "\n",
        "    tissue_image = tf.image.decode_png(tissue_image, channels=3)\n",
        "    mask_image = tf.image.decode_png(mask_image, channels=1)\n",
        "\n",
        "    tissue_image = tf.image.resize(tissue_image, [512, 512])\n",
        "    mask_image = tf.image.resize(mask_image, [512, 512])\n",
        "\n",
        "    # _, mask_image = cv2.threshold(mask_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    tissue_image = tf.cast(tissue_image, tf.float32) / 255.0\n",
        "    mask_image = tf.cast(mask_image, tf.float32) / 255.0\n",
        "\n",
        "    return tissue_image, mask_image\n",
        "\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((tissue_train, mask_train))\n",
        "  dataset = dataset.map(parse_images)\n",
        "  dataset = dataset.batch(batch)\n",
        "  dataset = dataset.repeat()\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_gaussian(tissue_train, mask_train, batch, noise_stddev=0.01):\n",
        "    def parse_images(tissue_train, mask_train):\n",
        "        tissue_file_str = tf.strings.join([train_path, \"TissueImages/\", tissue_train])\n",
        "        mask_file_str = tf.strings.join([train_path, \"GroundTruth/\", mask_train])\n",
        "\n",
        "        tissue_image = tf.io.read_file(tissue_file_str)\n",
        "        mask_image = tf.io.read_file(mask_file_str)\n",
        "\n",
        "        tissue_image = tf.image.decode_png(tissue_image, channels=3)\n",
        "        mask_image = tf.image.decode_png(mask_image, channels=1)\n",
        "\n",
        "        tissue_image = tf.image.resize(tissue_image, [512, 512])\n",
        "        mask_image = tf.image.resize(mask_image, [512, 512])\n",
        "\n",
        "        tissue_image = tf.cast(tissue_image, tf.float32) / 255.0\n",
        "        mask_image = tf.cast(mask_image, tf.float32) / 255.0\n",
        "\n",
        "        noise = tf.random.normal(shape=tf.shape(tissue_image), mean=0.0, stddev=noise_stddev)\n",
        "        tissue_image = tissue_image + noise\n",
        "        tissue_image = tf.clip_by_value(tissue_image, 0.0, 1.0)\n",
        "\n",
        "        return tissue_image, mask_image\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((tissue_train, mask_train))\n",
        "    dataset = dataset.map(parse_images)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.repeat()\n",
        "\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "OOeCI7qHU-in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtlIDxaYxxuv"
      },
      "outputs": [],
      "source": [
        "def create_test(tissue_test, mask_test,batch):\n",
        "  def parse_images(tissue_test, mask_test):\n",
        "\n",
        "    tissue_file_str = tf.strings.join([test_path, \"TissueImages/\", tissue_test])\n",
        "    mask_file_str = tf.strings.join([test_path, \"GroundTruth/\", mask_test])\n",
        "\n",
        "    tissue_image = tf.io.read_file(tissue_file_str)\n",
        "    mask_image = tf.io.read_file(mask_file_str)\n",
        "\n",
        "    tissue_image = tf.image.decode_png(tissue_image, channels=3)\n",
        "    mask_image = tf.image.decode_png(mask_image, channels=1)\n",
        "\n",
        "    tissue_image = tf.image.resize(tissue_image, [512, 512])\n",
        "    mask_image = tf.image.resize(mask_image, [512, 512])\n",
        "\n",
        "    tissue_image = tf.cast(tissue_image, tf.float32) / 255.0\n",
        "    mask_image = tf.cast(mask_image, tf.float32) / 255.0\n",
        "\n",
        "\n",
        "    return tissue_image, mask_image\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((tissue_test, mask_test))\n",
        "  dataset = dataset.map(parse_images)\n",
        "  dataset = dataset.batch(batch)\n",
        "  dataset = dataset.repeat()\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyI0OPFh_2uF"
      },
      "outputs": [],
      "source": [
        "TOTAL_TRAIN_SAMPLES = len(tissue_train)\n",
        "TOTAL_TEST_SAMPLES = len(tissue_test)\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "train_dataset = create_train(tissue_train, mask_train,BATCH_SIZE)\n",
        "test_dataset = create_test(tissue_test, mask_test,BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li1ZhxluCrDQ"
      },
      "source": [
        "**MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msAcLRItFXSz"
      },
      "source": [
        "*Graph Layer*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import register_keras_serializable"
      ],
      "metadata": {
        "id": "RmPgR-DNI00o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4qITXXO1H92"
      },
      "outputs": [],
      "source": [
        "@register_keras_serializable(package=\"MyLayers\")\n",
        "class SqueezeExcitation(tf.keras.layers.Layer):\n",
        "    def __init__(self, reduction_ratio=16, **kwargs):\n",
        "        super(SqueezeExcitation, self).__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.global_pooling = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        # These will be initialized in build()\n",
        "        self.squeeze_conv = None\n",
        "        self.excitation_conv = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        c = input_shape[-1]\n",
        "        self.squeeze_conv = tf.keras.layers.Conv2D(\n",
        "            filters=c // self.reduction_ratio,\n",
        "            kernel_size=(1, 1),\n",
        "            activation='relu',\n",
        "            kernel_initializer='he_normal',\n",
        "            use_bias=False\n",
        "        )\n",
        "        self.excitation_conv = tf.keras.layers.Conv2D(\n",
        "            filters=c,\n",
        "            kernel_size=(1, 1),\n",
        "            activation='sigmoid',\n",
        "            kernel_initializer='he_normal',\n",
        "            use_bias=False\n",
        "        )\n",
        "        super(SqueezeExcitation, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.global_pooling(inputs)\n",
        "        x = tf.keras.layers.Reshape((1, 1, inputs.shape[-1]))(x)\n",
        "        x = self.squeeze_conv(x)\n",
        "        x = self.excitation_conv(x)\n",
        "        return inputs * x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(SqueezeExcitation, self).get_config()\n",
        "        config.update({'reduction_ratio': self.reduction_ratio})\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnPjGV1OeKRK"
      },
      "outputs": [],
      "source": [
        "class WeightLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(WeightLayer, self).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    # g_init = tf.random_normal_initializer()\n",
        "      # w_init = tf.constant_initializer(1.0)\n",
        "      self.weight_layer = keras.layers.Conv2D(\n",
        "            filters = input_shape[-1],\n",
        "            kernel_size = (1, 1),\n",
        "            strides = (1, 1),\n",
        "            padding = \"same\"\n",
        "        )\n",
        "      super(WeightLayer, self).build(input_shape)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      w = self.weight_layer(inputs)\n",
        "      weighted_inp = tf.multiply(inputs, w)\n",
        "      return weighted_inp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0GJgCYDpCn-"
      },
      "outputs": [],
      "source": [
        "class NewGraphLayer:\n",
        "\n",
        "  def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.mask_matrix = tf.tile( tf.eye(n), [1, n])\n",
        "        self.unmask_matrix = tf.constant([[int(j // n == i) for j in range(n ** 2)] for i in range(n)],dtype=tf.float32)\n",
        "        self.unflatten_mat = tf.transpose(tf.reshape(tf.eye(n)*tf.expand_dims(tf.expand_dims(tf.eye(n), axis = -1), axis = -1), (n, n*n, n)), perm = [2,1,0])\n",
        "        # self.kl = weight_layer\n",
        "\n",
        "  def custom_flatten(self,A):\n",
        "        \"\"\"\n",
        "        A : (B, n,n, c)\n",
        "        C : (B, n^2, c)\n",
        "        \"\"\"\n",
        "        n = A.shape[1]\n",
        "        B = tf.transpose(A, perm=[0,3,1,2])\n",
        "        B = (B @  self.mask_matrix)*self.unmask_matrix\n",
        "        B = tf.reduce_sum(B,axis=-2)\n",
        "        C = tf.transpose(B, perm=[0,2,1])\n",
        "        return C\n",
        "\n",
        "  def custom_unflatten(self, A):\n",
        "      \"\"\"\n",
        "      A : (B, n^2)\n",
        "      C : (B, N, N)\n",
        "      \"\"\"\n",
        "      C = tf.transpose((tf.matmul(A , self.unflatten_mat)),perm=[1,2,0])\n",
        "      return C\n",
        "\n",
        "  def sum_channels(self, flattened_nodes):\n",
        "        \"\"\"\n",
        "        flattened_nodes : (B, N^2, C)\n",
        "        x : (B, C)\n",
        "        \"\"\"\n",
        "        x = tf.reduce_sum(flattened_nodes, axis = 1)\n",
        "        x = tf.expand_dims(x, axis = 1)\n",
        "        return x\n",
        "\n",
        "  def compute_dot_products(self):\n",
        "        \"\"\"\n",
        "        summed_vectors: (B, 1, C)\n",
        "        node_features = (B, N^2, C)\n",
        "        dot_products = (B, N^2)\n",
        "        \"\"\"\n",
        "        dot_products = tf.reduce_sum(tf.multiply(self.node_features, self.summed_vectors), axis = -1)\n",
        "        return dot_products\n",
        "\n",
        "  def prune_channel_by_variance(self, feature_map): # feature_map = B x N x N x C\n",
        "        variance_per_channel_vector = tf.math.reduce_variance(feature_map, axis = (1, 2), keepdims = True) # B x 1 x 1 x C\n",
        "        # print(variance_per_channel_vector)\n",
        "        # scaled_variance_per_channel_vector =  variance_per_channel_vector/ tf.reduce_max(variance_per_channel_vector, axis = (1,2,3), keepdims = True)\n",
        "        # print(scaled_variance_per_channel_vector)\n",
        "        mean_variance_per_sample = tf.reduce_mean(variance_per_channel_vector, axis = (1,2,3), keepdims = True) # (B,)\n",
        "        # relu_mask = tf.keras.activations.relu((scaled_variance_per_channel_vector-mean_variance_per_sample)/tf.reduce_max(scaled_variance_per_channel_vector-mean_variance_per_sample, axis = (1,2,3), keepdims = True))\n",
        "        # relu_mask_scaled = relu_mask / tf.reduce_max(relu_mask, axis = (1,2,3), keepdims = True)\n",
        "        # print(mean_variance_per_sample)\n",
        "        pruning_mask = tf.where(variance_per_channel_vector > mean_variance_per_sample, 1.0, 0.0)\n",
        "        # print(relu_mask_scaled)\n",
        "        # pruned_feature_map = feature_map * relu_mask_scaled\n",
        "        pruned_feature_map = feature_map * pruning_mask\n",
        "\n",
        "        # print(pruned_feature_map)\n",
        "        return pruned_feature_map\n",
        "\n",
        "  def create_graph_map(self, dot_products):\n",
        "        map = self.custom_unflatten(dot_products)\n",
        "        # map = tf.cast(map, tf.float32) / tf.reduce_max(map)\n",
        "        # scaled_map = (map - tf.reduce_min(map)) / (tf.reduce_max(map) - tf.reduce_min(map))\n",
        "        # min_values = tf.reduce_min(map, axis=(1, 2), keepdims=True)\n",
        "        max_values = tf.reduce_max(map, axis=(1, 2), keepdims=True)\n",
        "        # scaled_map = (map - min_values) / (max_values - min_values)\n",
        "        map = tf.cast(map, tf.float32) / (max_values)\n",
        "        return 3. * map\n",
        "\n",
        "  def __call__(self, input_data):\n",
        "        self.input_data = input_data\n",
        "        self.pruned_data = self.prune_channel_by_variance(input_data)\n",
        "        self.node_features =self.custom_flatten(self.pruned_data)\n",
        "        self.summed_vectors = self.sum_channels(self.node_features)\n",
        "        dot_products = self.compute_dot_products()\n",
        "        # kernel_similarity_scores = self.kl(self.node_features)\n",
        "        map = self.create_graph_map(dot_products)\n",
        "        return map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq_BhPwyFM6A"
      },
      "source": [
        "**Attention layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i2LEtPUBlxV"
      },
      "outputs": [],
      "source": [
        "@register_keras_serializable(package=\"MyLayers\")\n",
        "class GraphAttentionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, n, **kwargs):\n",
        "        self.n = n\n",
        "        super(GraphAttentionLayer, self).__init__(**kwargs)\n",
        "    # @tf.function\n",
        "    def build(self, input_shape):\n",
        "        self.phi_g = Conv2D(\n",
        "            filters = input_shape[-1],\n",
        "            kernel_size = (1, 1),\n",
        "            strides = (1, 1),\n",
        "            padding = \"same\"\n",
        "        )\n",
        "        self.theta_x = Conv2D(\n",
        "            filters = input_shape[-1],\n",
        "            kernel_size = (1,1),\n",
        "            strides = (1,1),\n",
        "            padding = \"same\"\n",
        "        )\n",
        "        self.concatenate = Concatenate(axis = -1)\n",
        "        self.psi = Conv2D(\n",
        "            filters = 1,\n",
        "            kernel_size = (1,1),\n",
        "            padding = \"same\"\n",
        "        )\n",
        "        self.result = Conv2D(\n",
        "            filters = input_shape[-1],\n",
        "            kernel_size = (1,1),\n",
        "            padding = \"same\"\n",
        "        )\n",
        "        self.multiply = tf.keras.layers.Multiply()\n",
        "        self.bn =  tf.keras.layers.BatchNormalization()\n",
        "        self.up = UpSampling2D(size = (2, 2))\n",
        "        self.wl = WeightLayer()\n",
        "        super(GraphAttentionLayer, self).build(input_shape)\n",
        "\n",
        "    # @tf.function\n",
        "    def call(self, input_x, input_g):\n",
        "        _, h, w, c = input_x.shape\n",
        "        x = tf.reshape(input_x, (tf.shape(input_x)[0], h//2, w//2, c * 4))\n",
        "        x = self.theta_x(x)\n",
        "        g = self.phi_g(input_g)\n",
        "\n",
        "        concat_inputs = self.concatenate([x, g])\n",
        "        concat_inputs_wl = self.wl(concat_inputs)\n",
        "        concat_inputs_activated = tf.keras.activations.relu(concat_inputs_wl)\n",
        "\n",
        "        a = NewGraphLayer(self.n)\n",
        "        map = a(concat_inputs_activated)\n",
        "        map_expanded = tf.expand_dims(map, axis = -1)\n",
        "        map_expanded_ = tf.keras.activations.sigmoid(map_expanded)\n",
        "        map_upsampled = self.up(map_expanded_)\n",
        "\n",
        "        y = self.multiply([input_x, map_upsampled])\n",
        "        y_res = self.result(y)\n",
        "\n",
        "        y_bn = self.bn(y_res)\n",
        "\n",
        "        return y_bn,map_upsampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSXfxqkMFeh4"
      },
      "source": [
        "*Instance normalization*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT-TQ6JFBq2W"
      },
      "outputs": [],
      "source": [
        "class InstanceNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, epsilon=1e-5):\n",
        "        super(InstanceNormalization, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create a scale parameter and a shift parameter for each channel\n",
        "        self.scale = self.add_weight(\n",
        "            name='scale',\n",
        "            shape=(input_shape[-1],),\n",
        "            initializer='ones',\n",
        "            trainable=True\n",
        "        )\n",
        "        self.shift = self.add_weight(\n",
        "            name='shift',\n",
        "            shape=(input_shape[-1],),\n",
        "            initializer='zeros',\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Calculate mean and variance for each channel independently\n",
        "        mean = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
        "        variance = tf.reduce_mean(tf.square(inputs - mean), axis=[1, 2], keepdims=True)\n",
        "\n",
        "        # Normalize the input\n",
        "        normalized = (inputs - mean) / tf.sqrt(variance + self.epsilon)\n",
        "\n",
        "        # Apply scale and shift\n",
        "        output = self.scale * normalized + self.shift\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM6BHEG-oyxt"
      },
      "outputs": [],
      "source": [
        "@register_keras_serializable(package=\"MyLayers\")\n",
        "class GroupNormalization(tf.keras.layers.Layer):\n",
        "    def __init__(self, groups=1, epsilon=1e-5, **kwargs):\n",
        "        super(GroupNormalization, self).__init__(**kwargs)\n",
        "        self.groups = groups\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Ensure that the number of channels is divisible by the number of groups\n",
        "        assert input_shape[-1] % self.groups == 0\n",
        "\n",
        "        # Create a scale parameter and a shift parameter for each group\n",
        "        self.scale = self.add_weight(\n",
        "            name='scale',\n",
        "            shape=(input_shape[-1],),\n",
        "            initializer='ones',\n",
        "            trainable=True\n",
        "        )\n",
        "        self.shift = self.add_weight(\n",
        "            name='shift',\n",
        "            shape=(input_shape[-1],),\n",
        "            initializer='zeros',\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, height, width, channels = input_shape[0], input_shape[1], input_shape[2], input_shape[3]\n",
        "\n",
        "        # Reshape inputs to [batch_size, height, width, groups, channels_per_group]\n",
        "        grouped_inputs = tf.reshape(inputs, [batch_size, height, width, self.groups, channels // self.groups])\n",
        "\n",
        "        # Calculate mean and variance for each group independently\n",
        "        mean = tf.reduce_mean(grouped_inputs, axis=[1, 2, 4], keepdims=True)\n",
        "        variance = tf.reduce_mean(tf.square(grouped_inputs - mean), axis=[1, 2, 4], keepdims=True)\n",
        "\n",
        "        # mean, variance = tf.nn.moments(grouped_inputs, [1, 2, 4], keepdims=True)\n",
        "\n",
        "        # Normalize the input within each group\n",
        "        normalized = (grouped_inputs - mean) / tf.sqrt(variance + self.epsilon)\n",
        "\n",
        "        # Reshape back to the original shape\n",
        "        normalized = tf.reshape(normalized, [batch_size, height, width, channels])\n",
        "\n",
        "        # Apply scale and shift\n",
        "        output = self.scale * normalized + self.shift\n",
        "        return output\n",
        "\n",
        "        def get_config(self):\n",
        "          config = super(GroupNormalization, self).get_config()\n",
        "          config.update({\n",
        "              'groups': self.groups,\n",
        "              'epsilon': self.epsilon,\n",
        "          })\n",
        "          return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYqcq-xNFwZo"
      },
      "source": [
        "*weighted GAP*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG495r_cBq2W"
      },
      "outputs": [],
      "source": [
        "class WeightedGlobalAveragePooling2D(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_channels, **kwargs):\n",
        "        super(WeightedGlobalAveragePooling2D, self).__init__(**kwargs)\n",
        "        self.num_channels = num_channels\n",
        "        # Create a trainable weight variable for each channel\n",
        "        self.channel_weights = self.add_weight(name='channel_weights',\n",
        "                                              shape=(num_channels,),\n",
        "                                              initializer='ones',\n",
        "                                              trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Calculate weighted global average pooling\n",
        "        weighted_sum = tf.reduce_sum(inputs * self.channel_weights, axis=[1, 2])\n",
        "        weighted_average = weighted_sum / tf.reduce_sum(self.channel_weights)\n",
        "        return weighted_average\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.num_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_PcL_kgBq2W"
      },
      "outputs": [],
      "source": [
        "def WGCAM(x):\n",
        "    num_filters = x.shape[-1]\n",
        "    wav = DWT.DWT(concat=0)(x)\n",
        "    wav = Conv2DTranspose(num_filters*4, (2, 2), strides=2, padding=\"same\")(wav)\n",
        "    wav = SeparableConv2D(num_filters, (1,1), padding=\"same\")(wav)\n",
        "    x_sam = SeparableConv2D(num_filters, (1,1), padding=\"same\")(x)\n",
        "    x_sam = wav+x_sam\n",
        "    x_sam = SeparableConv2D(num_filters, (1,1), padding=\"same\", activation='sigmoid')(x_sam)\n",
        "    x_cam = WeightedGlobalAveragePooling2D(num_filters)(x)\n",
        "    x_cam = Dense(num_filters/4, activation='relu')(x_cam)\n",
        "    x_cam = Dense(num_filters, activation='sigmoid')(x_cam)\n",
        "    x_cam = tf.keras.layers.Reshape((1, 1, x_cam.shape[-1]))(x_cam)\n",
        "    x = x*x_sam\n",
        "    x = tf.keras.layers.Multiply()([x, x_cam])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT9KQUdiBq2W"
      },
      "outputs": [],
      "source": [
        "def CombinedUpsampleLayer(inputs):\n",
        "    _,H,W,C = inputs.shape\n",
        "    # gaussian = UpSampling2D(size=(2, 2), interpolation=\"gaussian\")(inputs)\n",
        "    combined = UpSampling2D(size=(2, 2), interpolation=\"lanczos5\")(inputs)\n",
        "    # combined = tf.keras.layers.Add()([gaussian, lanczos])\n",
        "    combined_attn = Conv2D(C, 1, padding=\"same\")(combined)\n",
        "\n",
        "    # Assuming you want to upsample to the original input size\n",
        "    upsampled = Conv2DTranspose(C, (2, 2), strides=2, padding=\"same\")(inputs)\n",
        "\n",
        "    x = Concatenate()([combined_attn,upsampled])\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvNxlTJcQBmZ"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "class FunctionalModel:\n",
        "  def __init__(self):\n",
        "    self.pre_trained_backbone =  tf.keras.applications.DenseNet121(\n",
        "      include_top=False ,\n",
        "      weights='imagenet' ,\n",
        "      input_shape=(512, 512, 3))\n",
        "\n",
        "  def conv_block(self,inputs, num_filters):\n",
        "    x1 = Conv2D(num_filters//2, 5, padding=\"same\")(inputs)\n",
        "    x1 = GroupNormalization()(x1)\n",
        "    x1 = Activation(\"relu\")(x1)\n",
        "\n",
        "    x2 = Conv2D(num_filters//2, 3, padding=\"same\")(inputs)\n",
        "    x2 = GroupNormalization()(x2)\n",
        "    x2 = Activation(\"relu\")(x2)\n",
        "\n",
        "    x2 = Concatenate()([x1,x2])\n",
        "    x2 = Conv2D(num_filters, 1, padding=\"same\")(x2)\n",
        "\n",
        "    x3 = Conv2D(num_filters, 1, padding=\"same\")(inputs)\n",
        "    x3 = GroupNormalization()(x3)\n",
        "    x3 = Activation(\"relu\")(x3)\n",
        "\n",
        "    x3 = Concatenate()([x2,x3])\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x3)\n",
        "    x = GroupNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "  def bridge_layer(self, x, num_filters):\n",
        "    x_init = x\n",
        "\n",
        "    x = Conv2D(num_filters, 4, padding=\"same\")(x)\n",
        "    x = GroupNormalization(groups=4)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 4, padding=\"same\")(x)\n",
        "    x = GroupNormalization(groups=4)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "  def decoder_block(self,inputs, skip_features, num_filters):\n",
        "    x = CombinedUpsampleLayer(inputs)\n",
        "    skip_features = Activation(\"relu\")(skip_features)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = self.conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "  def get(self):\n",
        "\n",
        "    inputs = self.pre_trained_backbone.input\n",
        "\n",
        "    s1 = 'input_1'\n",
        "    s2 = 'conv1_relu'\n",
        "    s3 = 'pool2_relu'\n",
        "    s4 = 'pool3_relu'\n",
        "\n",
        "    # Encoder\n",
        "    s1 = inputs\n",
        "    s2 = self.pre_trained_backbone.get_layer(s2).output #256\n",
        "    s3 = self.pre_trained_backbone.get_layer(s3).output #128\n",
        "    s4 = self.pre_trained_backbone.get_layer(s4).output #64\n",
        "    b = self.pre_trained_backbone.get_layer(\"pool4_relu\").output  ## 32\n",
        "\n",
        "    x1 = SqueezeExcitation()(s4)\n",
        "    G11, map11 = GraphAttentionLayer(n=32, name = \"Graph11_64\")(x1, b)\n",
        "    G12, map12 = GraphAttentionLayer(n=32, name = \"Graph12_64\")(x1, b)\n",
        "    G13, map13 = GraphAttentionLayer(n=32, name = \"Graph13_64\")(x1, b)\n",
        "\n",
        "    G1 = Concatenate(axis=-1)([G11, G12, G13])\n",
        "\n",
        "    # Decoder\n",
        "    d1 = self.decoder_block(b, G1, 512) # 32 -> b gets upsampled to 64 through CombinedUpsampleLayer() # 64\n",
        "\n",
        "    # Graph attention 1 to d2 with dimension 64\n",
        "    x2 = SqueezeExcitation()(s3)\n",
        "    G21, map21 = GraphAttentionLayer(n=64, name = \"Graph21_64\")(x2, d1)\n",
        "    G22, map22 = GraphAttentionLayer(n=64, name = \"Graph22_64\")(x2, d1)\n",
        "    G23, map23 = GraphAttentionLayer(n=64, name = \"Graph23_64\")(x2, d1)\n",
        "\n",
        "    # G2 = Concatenate([G21, G22, G23], axis = -1)\n",
        "    G2 = Concatenate(axis=-1)([G21, G22, G23])\n",
        "    # G2 =  SqueezeExcitation()(G2)\n",
        "\n",
        "    d2 = self.decoder_block(d1, G2, 256)                               # 128\n",
        "\n",
        "    # Graph attention 2 to d3 with dimension 256\n",
        "    x3 = SqueezeExcitation()(s2)\n",
        "    G31, map31 = GraphAttentionLayer(n=128, name = \"Graph31_256\")(x3, d2)\n",
        "    G32, map32 = GraphAttentionLayer(n=128, name = \"Graph32_256\")(x3, d2)\n",
        "    G33, map33 = GraphAttentionLayer(n=128, name = \"Graph33_256\")(x3, d2)\n",
        "\n",
        "    # G3 = Concatenate([G31, G32, G33], axis = -1)\n",
        "    G3 = Concatenate(axis=-1)([G31, G32, G33])\n",
        "\n",
        "    d3 = self.decoder_block(d2, G3, 128)                             # 256\n",
        "    d4 = self.decoder_block(d3, s1, 64)                                 # 512\n",
        "\n",
        "\n",
        "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "    # map1 = s4\n",
        "    model = Model(inputs=inputs, outputs=[outputs, s3, s4, map11, map12, map13, map21, map22, map23, map31, map32, map33])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRBCX9zXJTnd",
        "outputId": "b62b6949-628d-4f01-c51e-45ae44153163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_unet_collection in /usr/local/lib/python3.10/dist-packages (0.1.13)\n"
          ]
        }
      ],
      "source": [
        "smooth = 1e-3\n",
        "\n",
        "class DiceCoeff:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def dice_coef(self, y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    return self.dice_coef(y_true, y_pred)\n",
        "\n",
        "class Precision(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='precision', **kwargs):\n",
        "        super(Precision, self).__init__(name=name, **kwargs)\n",
        "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
        "        self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.cast(y_true, tf.bool)\n",
        "        y_pred = tf.cast(y_pred, tf.bool)\n",
        "\n",
        "        true_positives = tf.reduce_sum(tf.cast(tf.logical_and(y_true, y_pred), tf.float32))\n",
        "        predicted_positives = tf.reduce_sum(tf.cast(y_pred, tf.float32))\n",
        "\n",
        "        self.true_positives.assign_add(true_positives)\n",
        "        self.predicted_positives.assign_add(predicted_positives)\n",
        "\n",
        "    def result(self):\n",
        "        return self.true_positives / (self.predicted_positives + K.epsilon())\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.true_positives.assign(0)\n",
        "        self.predicted_positives.assign(0)\n",
        "\n",
        "\n",
        "class Recall(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='recall', **kwargs):\n",
        "        super(Recall, self).__init__(name=name, **kwargs)\n",
        "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
        "        self.possible_positives = self.add_weight(name='pp', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.cast(y_true, tf.bool)\n",
        "        y_pred = tf.cast(y_pred, tf.bool)\n",
        "\n",
        "        true_positives = tf.reduce_sum(tf.cast(tf.logical_and(y_true, y_pred), tf.float32))\n",
        "        possible_positives = tf.reduce_sum(tf.cast(y_true, tf.float32))\n",
        "\n",
        "        self.true_positives.assign_add(true_positives)\n",
        "        self.possible_positives.assign_add(possible_positives)\n",
        "\n",
        "    def result(self):\n",
        "        return self.true_positives / (self.possible_positives + K.epsilon())\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.true_positives.assign(0)\n",
        "        self.possible_positives.assign(0)\n",
        "\n",
        "\n",
        "class F1Score(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = Precision()\n",
        "        self.recall = Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.reshape(y_true, [-1])\n",
        "        y_pred = tf.reshape(y_pred, [-1])\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.precision.result()\n",
        "        recall = self.recall.result()\n",
        "        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "\n",
        "    def reset_states(self):\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()\n",
        "\n",
        "\n",
        "class VOE:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def voe_metric(self, y_true, y_pred):\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
        "    voe = 1.0 - (intersection / union)\n",
        "    return voe\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    return self.voe_metric(y_true, y_pred)\n",
        "\n",
        "!pip install keras_unet_collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNdGcXid_qg4",
        "outputId": "e80d2f40-bf81-43a1-932a-4384ccf14216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hausdorff in /usr/local/lib/python3.10/dist-packages (0.2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install hausdorff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv6dJ_u5JXV4"
      },
      "outputs": [],
      "source": [
        "from keras_unet_collection import losses\n",
        "from hausdorff import hausdorff_distance\n",
        "\n",
        "class DiceLoss:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def dice_loss(self, y_true, y_pred):\n",
        "    return 1.0 - DiceCoeff()(y_true, y_pred)\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    return self.dice_loss(y_true, y_pred)\n",
        "\n",
        "class HybridLoss:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def hybrid_loss(self, y_true, y_pred):\n",
        "    loss_focal = losses.focal_tversky(y_true, y_pred, alpha=0.3, gamma=4/3)\n",
        "    loss_dice = DiceLoss()(y_true, y_pred)\n",
        "    return loss_focal+loss_dice\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    return self.hybrid_loss(y_true, y_pred)\n",
        "\n",
        "class CustomLoss:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def custom_loss(self, y_true, y_pred):\n",
        "    loss_dice = DiceLoss()(y_true, y_pred)\n",
        "    loss_voe = VOE()(y_true, y_pred)\n",
        "    return loss_voe + loss_dice\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    return self.custom_loss(y_true, y_pred)\n",
        "\n",
        "class BCELoss:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def bce_loss(self, y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n",
        "    return loss\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    return self.bce_loss(y_true, y_pred)\n",
        "\n",
        "class CombinedLoss:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def combined_loss(self, y_true, y_pred):\n",
        "    loss = DiceLoss()(y_true, y_pred) + BCELoss()(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    return self.combined_loss(y_true, y_pred)\n",
        "\n",
        "class HausdorffLoss:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def euclidean_distance(self, x, y):\n",
        "        # Compute pairwise Euclidean distance between two sets of points\n",
        "        return tf.sqrt(tf.reduce_sum(tf.square(tf.expand_dims(x, 1) - tf.expand_dims(y, 0)), axis=-1))\n",
        "\n",
        "    def hausdorff_distance(self, x, y):\n",
        "        # Compute pairwise distances\n",
        "        distances_x_to_y = self.euclidean_distance(x, y)\n",
        "        distances_y_to_x = self.euclidean_distance(y, x)\n",
        "\n",
        "        # Calculate Hausdorff distance\n",
        "        hausdorff_distance = tf.reduce_max(tf.reduce_min(distances_x_to_y, axis=1))\n",
        "        hausdorff_distance = tf.maximum(hausdorff_distance, tf.reduce_max(tf.reduce_min(distances_y_to_x, axis=1)))\n",
        "\n",
        "        return hausdorff_distance\n",
        "\n",
        "    def hausdorff_loss(self, pmask, gtmask):\n",
        "        pmask1 = tf.squeeze(pmask[0])\n",
        "        pmask2 = tf.squeeze(pmask[1])\n",
        "        gtmask1 = tf.squeeze(gtmask[0])\n",
        "        gtmask2 = tf.squeeze(gtmask[1])\n",
        "\n",
        "        # Calculate Hausdorff distance between the masks\n",
        "        loss1 = self.hausdorff_distance(pmask1, gtmask1)\n",
        "        loss2 = self.hausdorff_distance(pmask2, gtmask2)\n",
        "\n",
        "        # Return the average Hausdorff loss\n",
        "        return (loss1 + loss2) / 2\n",
        "\n",
        "    def __call__(self, pmask, gtmask):\n",
        "        return self.hausdorff_loss(pmask, gtmask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PdkJj2Ieazp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CustomModelWrapper:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.prev_model = model\n",
        "        self.t_steps_per_epoch = TOTAL_TRAIN_SAMPLES // BATCH_SIZE\n",
        "        self.v_steps_per_epoch = TOTAL_TEST_SAMPLES // BATCH_SIZE\n",
        "        self.cur_epoch = 0\n",
        "        self.best_model = {'score': 0, 'model': self.model, 'result': None, 'metric_vals': None}\n",
        "        self.history = {}\n",
        "\n",
        "    def compile(self, loss_objs: dict, optimizer_obj, metrics: dict):\n",
        "        self.loss_objs, self.optimizer_obj, self.metrics = loss_objs, optimizer_obj, metrics\n",
        "\n",
        "    @tf.function\n",
        "    def train_single_batch(self, x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds, s3, s4, map11, map12, map13, map21, map22, map23, map31, map32, map33 = self.model(x, training=True)\n",
        "            custom_loss_value = self.loss_objs['custom_loss'](y, preds)\n",
        "            dice_loss = self.loss_objs['dice_loss'](y, preds)\n",
        "            hausdorff_loss = self.loss_objs['hausdorff_loss'](preds, y) / 30\n",
        "            loss_value = custom_loss_value + hausdorff_loss\n",
        "\n",
        "        grads = tape.gradient(loss_value, self.model.trainable_weights)\n",
        "        self.optimizer_obj.apply_gradients(zip(grads, self.model.trainable_weights))\n",
        "        return preds, loss_value, dice_loss, hausdorff_loss, custom_loss_value\n",
        "\n",
        "    def train_single_epoch(self, data):\n",
        "        losses = []\n",
        "        dice_scores = []\n",
        "        hausdorff_loss = []\n",
        "        custom_loss = []\n",
        "\n",
        "        pbar = tqdm(total=self.t_steps_per_epoch, position=0, leave=True,\n",
        "                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ')\n",
        "\n",
        "        for step in range(1, self.t_steps_per_epoch + 1):\n",
        "            (x, y) = next(data)\n",
        "            preds, loss_value, dice_loss, hloss, custom_loss_value = self.train_single_batch(x, y)\n",
        "\n",
        "            losses.append(loss_value)\n",
        "            hausdorff_loss.append(hloss)\n",
        "            custom_loss.append(custom_loss_value)\n",
        "            dice_score = self.metrics['train_dice'](y, preds)\n",
        "            if tf.reduce_max(y * preds) > 0:\n",
        "                dice_scores.append(dice_score)\n",
        "\n",
        "            pbar.set_description(\n",
        "                f\"Training loss: {loss_value:.4f}, Training Custom loss: {custom_loss_value:.4f}, Training Dice loss: {dice_loss:.4f}, Hloss: {hloss:.4f} for step: {step}\")\n",
        "            pbar.update()\n",
        "        pbar.close()\n",
        "        return losses, dice_scores, hausdorff_loss, custom_loss\n",
        "\n",
        "    def validate_single_epoch(self, data):\n",
        "        losses = []\n",
        "        val_dice_scores = []\n",
        "\n",
        "        for step in range(self.v_steps_per_epoch):\n",
        "            (x, y) = next(data)\n",
        "            preds, s4, s3, map11, map12, map13, map21, map22, map23, map31, map32, map33 = self.model(x)\n",
        "\n",
        "            loss_value = self.loss_objs['custom_loss'](y, preds)\n",
        "            losses.append(loss_value)\n",
        "\n",
        "            val_dice_score = self.metrics['val_dice'](y, preds)\n",
        "            if tf.reduce_max(y * preds) > 0:\n",
        "                val_dice_scores.append(val_dice_score)\n",
        "\n",
        "        return losses, val_dice_scores\n",
        "\n",
        "    def fit(self, train_data, val_data, epochs):\n",
        "        train_data_iter = iter(train_data)  # Convert train_data to an iterator\n",
        "        val_data_iter = iter(val_data)\n",
        "\n",
        "        history = {'train_loss': [], 'val_loss': [], 'train_dice_loss': [], 'train_dice': [], 'val_dice': [], 'hausdorff_loss': [], 'custom_loss': []}\n",
        "        for epoch in range(epochs):\n",
        "            train_losses, train_dice_scores, hloss, closs = self.train_single_epoch(train_data_iter)\n",
        "            train_dice_result = np.mean(train_dice_scores)\n",
        "            train_hloss = np.mean(hloss)\n",
        "            train_custom_loss = np.mean(closs)\n",
        "            history['hausdorff_loss'].append(train_hloss)\n",
        "            history['train_dice'].append(train_dice_result)\n",
        "            history['custom_loss'].append(train_custom_loss)\n",
        "\n",
        "            val_losses, val_dice_scores = self.validate_single_epoch(val_data_iter)\n",
        "            val_dice_result = np.mean(val_dice_scores)\n",
        "            history['val_dice'].append(val_dice_result)\n",
        "\n",
        "            history['train_loss'].append(np.mean(train_losses))\n",
        "            history['val_loss'].append(np.mean(val_losses))\n",
        "\n",
        "            print(\n",
        "                f'\\n Epoch {epoch}: Train loss: {np.mean(train_losses):.4f}, Validation Loss: {np.mean(val_losses):.4f}, Train Dice: {train_dice_result:.4f}, Validation Dice: {val_dice_result:.4f}, Train Custom Loss: {train_custom_loss:.4f}, Train Hloss: {train_hloss:.4f}')\n",
        "\n",
        "            self.cur_epoch += 1\n",
        "            self.prev_model = self.model\n",
        "\n",
        "            if self.cur_epoch % 5 == 0:\n",
        "                # Display the intermediate feature maps in a 3x3 grid\n",
        "                out, s3, s4, map11, map12, map13, map21, map22, map23, map31, map32, map33 = self.model(tissue, training=False)\n",
        "\n",
        "                feature_maps = [map11, map12, map13, map21, map22, map23, map31, map32, map33, out, out, out]\n",
        "\n",
        "                plt.figure(figsize=(15, 10))\n",
        "                for i, fmap in enumerate(feature_maps):\n",
        "                    plt.subplot(3, 4, i + 1)\n",
        "                    plt.imshow(fmap[1, :, :, 0], cmap='gray')\n",
        "                    plt.title(f'Map {i // 4 + 1}-{i % 4 + 1}')\n",
        "                plt.show()\n",
        "\n",
        "            if self.best_model['score'] < val_dice_result:\n",
        "                print(\"Storing new best ...\")\n",
        "                self.best_model['score'] = val_dice_result\n",
        "                self.best_model['model'] = self.model\n",
        "                self.best_model['metric_vals'] = {'val_dice_tf': val_dice_result}\n",
        "\n",
        "        history['model'] = self.model\n",
        "        self.history = history\n",
        "\n",
        "        return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJneVmCVJeq3"
      },
      "outputs": [],
      "source": [
        "model_wrapper = CustomModelWrapper(FunctionalModel().get())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXFle78DJgpf"
      },
      "outputs": [],
      "source": [
        "model_wrapper.compile(\n",
        "    loss_objs = {\n",
        "        'hybrid_loss':HybridLoss(),\n",
        "        'custom_loss':HybridLoss(),\n",
        "        'dice_loss': DiceLoss(),\n",
        "        'hausdorff_loss' : HausdorffLoss()\n",
        "        },\n",
        "    optimizer_obj =  tf.keras.optimizers.Nadam(learning_rate = 0.0003),\n",
        "    metrics = {\n",
        "        'train_acc':tf.keras.metrics.Accuracy(),\n",
        "        'val_acc':tf.keras.metrics.Accuracy(),\n",
        "        'train_dice':DiceCoeff(),\n",
        "        'val_dice': DiceCoeff(),\n",
        "        'train_f1': F1Score(),\n",
        "        'val_f1': F1Score(),\n",
        "        'train_rec': Recall(),\n",
        "        'val_rec': Recall(),\n",
        "        'train_pre': Precision(),\n",
        "        'val_pre': Precision()\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbSzvxIBJitn",
        "outputId": "957a7209-949f-42f5-b787-5078ae656557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/15 "
          ]
        }
      ],
      "source": [
        "history = model_wrapper.fit(\n",
        "    train_dataset,\n",
        "    test_dataset,\n",
        "    epochs=50\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}